{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pr√©traitement du son\n",
    "\n",
    "Comment convertir du son en features ? That is the question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cr√©ation d'un WAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done already\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "audio_path = \"images/output.wav\"\n",
    "if not os.path.exists(audio_path):\n",
    "    import sounddevice as sd\n",
    "    from scipy.io.wavfile import write\n",
    "    import numpy as np\n",
    "\n",
    "    # Param√®tres d'enregistrement\n",
    "    samplerate = 16000  # 16kHz (standard pour ML et ASR)\n",
    "    duration = 5  # Dur√©e de l'enregistrement en secondes\n",
    "\n",
    "    print(\"üé§ Enregistrement en cours...\")\n",
    "    audio = sd.rec(\n",
    "        int(samplerate * duration), samplerate=samplerate, channels=1, dtype=np.int16\n",
    "    )\n",
    "    sd.wait()  # Attendre la fin de l'enregistrement\n",
    "\n",
    "    # Sauvegarder en fichier .wav\n",
    "\n",
    "    write(audio_path, samplerate, audio)\n",
    "    print(\"done\")\n",
    "else:\n",
    "    print(\"done already\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## librosa\n",
    "\n",
    "[librosa](https://librosa.org/doc/latest/index.html) : le son est de longueur variable et il faut construire un vecteur de taille finie afin de pouvoir l'utiliser avec un pr√©dicteur. On d√©coupe le signal, on estime des features, puis on aggr√®ge d'une fa√ßon ou d'une autre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chargement du fichier: 'images/output.wav'\n",
      "features: 'images/output.wav'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13, 157)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# Charger un fichier audio\n",
    "print(f\"chargement du fichier: {audio_path!r}\")\n",
    "y, sr = librosa.load(\n",
    "    audio_path, sr=None\n",
    ")  # sr=None garde le taux d'√©chantillonnage original\n",
    "\n",
    "# Extraction des caract√©ristiques audio (MFCCs)\n",
    "print(f\"features: {audio_path!r}\")\n",
    "mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "mfccs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfccs = np.mean(mfccs, axis=1)\n",
    "mfccs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pr√©traitement avanc√© ou transfer learning\n",
    "\n",
    "Cette m√©thode s'apparente √† une [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning). Quand on dispose de peu de donn√©es, il est difficile d'apprendre un mod√®le performant sur des donn√©es complexes type image ou son. En revanche, on peut utiliser la sortie d'un mod√®le appris sur des grandes quantit√© de donn√©es et les utiliser comme feature. On parle d' *embedding*.\n",
    "\n",
    "Le package [transformers](https://huggingface.co/docs/transformers/en/index) offre plein de mod√®le de traitement de son, reconnaissance de la parole et autres traitements, il faut choisir un mod√®le qui s'approche de la t√¢che √† r√©aliser par la suite. L'exemple suivant consid√®re un petit mod√®le [distil-wav2vec2](https://huggingface.co/OthmaneJ/distil-wav2vec2) et transcrit le son en mots. Ce n'est pas le plus performant car c'est un petit mod√®le. On peut utiliser comme features la sortie du pr√©processeur, celle du mod√®le... Tout d√©pend de ce qui suit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t√©l√©chargement du preprocesseur 'openai/whisper-tiny'\n",
      "t√©l√©chargement du mod√®le 'openai/whisper-tiny'\n",
      "chargement du fichier audio 'images/output.wav'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 80, 3000]), torch.float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# Charger le mod√®le et le processeur\n",
    "model_name = \"openai/whisper-tiny\"\n",
    "print(f\"t√©l√©chargement du preprocesseur {model_name!r}\")\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "print(f\"t√©l√©chargement du mod√®le {model_name!r}\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "print(f\"chargement du fichier audio {audio_path!r}\")\n",
    "y, sr = librosa.load(\n",
    "    audio_path, sr=16000\n",
    ")  # wav2vec2 attend un √©chantillonnage de 16kHz\n",
    "\n",
    "# Transformer en tenseur\n",
    "input_features = processor(y, sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "input_features.shape, input_features.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 14]), torch.int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_ids = model.generate(input_features)\n",
    "predicted_ids.shape, predicted_ids.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte reconnu :  cr√©ation l'infiche-roix avec p'ton.\n"
     ]
    }
   ],
   "source": [
    "# D√©coder le texte\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "print(\"Texte reconnu :\", transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour avoir des features r√©els, on peut aussi appeler le mod√®le une seule fois mais le mod√®le n'aura peut-√™tre trait√© toute la s√©quence. Il faut comprendre le mod√®le avant d'utiliser ce code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 51865]), torch.float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_features = model(\n",
    "    input_features=input_features,\n",
    "    decoder_input_ids=torch.tensor([[50258]], dtype=torch.int64),\n",
    ")\n",
    "new_features.logits.shape, new_features.logits.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}