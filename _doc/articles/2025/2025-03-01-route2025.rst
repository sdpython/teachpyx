.. _l-feuille-route-2025:

=========================================
2025-03-01 : feuille de route 2025 - mars
=========================================

site web : `sdpython.github.io <https://sdpython.github.io/>`_

`Apprendre la programmation avec Python
<https://sdpython.github.io/doc/teachpyx/dev/>`_

Séance 1 (31/1)
===============

Un jeu de données :

* `Demandes de valeurs foncières
  <https://www.data.gouv.fr/fr/datasets/demandes-de-valeurs-foncieres/>`_

*Quatre thèmes*

* Deux types de problèmes : régression, classifications
* Problèmes dérivés : ranking, clustering, série temporelles
* Calculs, puissances de calculs, CUDA, CPU
* Environnement de travail, coder avec un LLM en local,
  :ref:`l-2025-01-31-local-llm`, avenir du datascientiste

*Autres notes*

* :ref:`l-ml-rappel`
* programmation (python :epkg:`numpy`, :epkg:`pandas`, :epkg:`matplotlib`, :epkg:`jupyter`)
* :ref:`Tests unitaires <nbl-practice-py-base-tests_unitaires>`, package python
* `SQL <https://en.wikipedia.org/wiki/SQL>`_
* `CPU <https://en.wikipedia.org/wiki/Central_processing_unit>`_,
  `CUDA <https://en.wikipedia.org/wiki/CUDA>`_
* machine learning, :epkg:`scikit-learn`, :epkg:`pytorch`
* `comparaison torch/scikit-learn <https://sdpython.github.io/doc/experimental-experiment/dev/auto_examples/plot_torch_linreg.html>`_
* :ref:`l-regclass`
* évaluation, :ref:`ROC <l-ml-plot-roc>`, :math:`R^2`
* ranking, clustering
* `ChatGPT <https://chat.openai.com/>`_,
  `LLM <https://en.wikipedia.org/wiki/Large_language_model>`_,
  (Large Language Model), SLLM (Small LLM)
* Coder avec un LLM en local : :ref:`l-2025-01-31-local-llm`
* Notebooks, Visual Studio Code

Séance 2 (7/2)
==============

* arbre de régression, de classification
  :ref:`RandomForest, Overfitting <nbl-practice-ml-ml_a_tree_overfitting>`
* Gradient Boosting, :ref:`Gradient Boosting et Learning Rate avec les Random Forest <nbl-practice-ml-gradient_boosting>`
* Régression linéaire et contraintes sur les coefficients,
  `Ridge <https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html>`_,
  `Lasso <https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html>`_,
  `ElasticNet <https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html>`_,
  :ref:`Ridge, Lasso, mathématiques <nbl-practice-ml-ridge_lasso>`
* paramètres et hyper paramètres, :ref:`Sélection des hyper-paramètres <nbl-practice-ml-winesr_knn_hyper>`,
  :class:`sklearn.model_selection.GridSearchCV`
* `Data challenge - Algorithme machine learning qui permet de prédire la gravité d’un accident de la rout
  <https://www.data.gouv.fr/fr/reuses/data-challenge-algorithme-machine-learning-qui-permet-de-predire-la-gravite-dun-accident-de-la-route/>`_

Séance 3 (14/2)
===============

*un peu plus de code*

* pipelines
* créer son propre estimateur
* tests unitaires

Pour cette séance, on souhaite construire un estimateur qui estime
une régression linéaire à coefficients positifs, une autre avec
des coefficients uniquement négatifs puis pour finir une dernière
régression linéaire qui considère les deux premières comme features.

Une régression linéaire minimise l'erreur
:math:`\sum_i \left\Vert X_i\theta - y_i \right\Vert^2`.
Le gradient est :math:`\sum_i X_i'\left( X_i\theta - y_i \right)`.

Comme le modèle souhaité est équivalent à une optimisation sous contrainte,
on propose de le résoudre comme ceci :

* On applique une itération de l'algorithme de la descente de gradient :
  :math:`\theta_{t+1} = \theta_t - \epsilon_t \sum_i X_i'\left( X_i\theta - y_i \right)`.
* On ne garde que les coefficients positifs : :math:`\theta_{t+1} = \max(0, \theta_t)`.
* On retourne à l'étape 1 ou on s'arrête si l'algorithme a convergé.

On appliquera cela au jeu de données :func:`sklearn.datasets.load_diabetes` ou
`Wine Quality <https://archive.ics.uci.edu/datasets?search=wine>`_
on comparera à une simple régression linéaire, les coefficients sont-ils
équivalents ? Comment comparer les modèles ?

* :ref:`Nouvel estimateur <nbl-practice-ml-custom_estimator>`

Séance 4 (21/2)
===============

* prétraitements
* anomalie
* cartes
* clustering

**Cartes**

* :ref:`Tracer une carte <nbl-c_data-enedis_cartes>`

**Réseaux de neurones**

* `Réseau de neurones <https://sdpython.github.io/doc/mlstatpy/dev/c_ml/rn/rn.html>`_,
  `LeNet <https://en.wikipedia.org/wiki/LeNet>`_
* `Seq2Seq <https://en.wikipedia.org/wiki/Seq2seq>`_,
  `Sequence To Sequence <https://paperswithcode.com/method/seq2seq>`_,
  `Sequence to Sequence (seq2seq) and Attention
  <https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html>`_,
  `Transformers <https://research.google/blog/transformer-a-novel-neural-network-architecture-for-language-understanding/>`_,
  `Attention is All You Need
  <https://france.devoteam.com/paroles-dexperts/attention-is-all-you-need-comprendre-le-traitement-naturel-du-langage-avec-les-modeles-transformers/>`_,
  `BLEU <https://fr.wikipedia.org/wiki/BLEU_(algorithme)>`_

**Anomalies**

* `Novelty and Outlier Detection <https://scikit-learn.org/stable/modules/outlier_detection.html>`_

**Clustering**

* `clustering <https://scikit-learn.org/stable/modules/clustering.html>`_
* Vieux notebooks sur l'utilisation de vélos à Chicago
  `City Bike Views <https://github.com/sdpython/ensae_projects/blob/master/_doc/notebooks/challenges/city_bike/city_bike_views.ipynb>`_,
  `City Bike Clustering <https://github.com/sdpython/ensae_projects/blob/master/_doc/notebooks/challenges/city_bike/city_bike_solution_cluster_start.ipynb>`_,

**Prétraitement**

* Dates, Catégories : :epkg:`category_encoders`, :epkg:`skrub`,
  :ref:`Prétraitement des catégories <nbl-practice-ml-pretraitement_cat>`
* Son : :epkg:`librosa`, voir :ref:`Prétraitement du son <nbl-practice-ml-pretraitement_son>`
* Image : :epkg:`scikit-image`, voir :ref:`Prétraitement d'une image <nbl-practice-ml-pretraitement_image>`
* Texte : :ref:`Prétraitement du texte <nbl-practice-ml-pretraitement_texte>`

Pour la suite, on souhaite comparer ces approches sur un jeu
accessible depuis le package `datasets <https://huggingface.co/docs/datasets/en/index>`_.
`Large Movie Review Dataset <https://ai.stanford.edu/~amaas/data/sentiment/>`_

.. code-block:: python

    from datasets import load_dataset

    dataset = load_dataset("stanfordnlp/imdb", split="train")

    print(dataset)
    Dataset({
        features: ['text', 'label'],
        num_rows: 25000
    })

Séance 5 (6/3)
==============

**Régression, Classification linéaires**

:epkg:`statsmodels` pour obtenir le résultat de tests de nullité des coefficients

* :ref:`Régression logistique en 2D <nbl-practice-ml-winesc_color_line>`
* :ref:`Plusieurs modèles, données disjointes <nbl-practice-ml-winesc_color_linear>`

**Interprétabilité**

* `Partial Dependence <https://scikit-learn.org/stable/modules/partial_dependence.html>`_
* `Permutation Importance <https://scikit-learn.org/stable/modules/permutation_importance.html>`_
* `LIME <https://arxiv.org/abs/1602.04938>`_
* `Shapley value <https://en.wikipedia.org/wiki/Shapley_value>`_,
  `SHAP <https://shap.readthedocs.io/en/latest/index.html>`_
* `Counterfactual Reasoning and Learning Systems <https://arxiv.org/abs/1209.2355>`_

**séries temporelles**

`Foundation Models for Time Series Analysis: A Tutorial and Survey <https://arxiv.org/pdf/2403.14735>`_

Le modèle de référence est :epkg:`statsmodels`

* :ref:`Single Spectrum Analysis (SSA) <nbl-practice-ml-timeseries_ssa>`
* :ref:`Décomposition d'une série temporelle <nbl-practice-ml-timeseries_seasonal>`

:epkg:`sktime` propose une API plus proche de :epkg:`scikit-learn`
et d'autres modèles comme le clusting ou la segmentation de séries temporelles.

:epkg:`prophet` fait aussi de la prédiction et contient aussi des algorithmes
de détection de changement de régime, il contient une bonne base de jours
fériés.

:epkg:`pyflux` permet d'estimer des modules `GARCH
<https://en.wikipedia.org/wiki/Autoregressive_conditional_heteroskedasticity>`_.

**Analyse de survie**

* :epkg:`scikit-survival`, :epkg:`lifelines`, analyses de survie,
  `Analyse de survie <https://sdpython.github.io/doc/mlstatpy/dev/c_ml/survival_analysis.html>`_,
  
**Deep Learning**

* `DeepAR <https://arxiv.org/abs/1704.04110>`_
  (code `Autoregressive modelling with DeepAR and DeepVAR
  <https://pytorch-forecasting.readthedocs.io/en/stable/tutorials/deepar.html#>`_)
* `Time Series Forecasting with LLMs: Understanding and Enhancing Model Capabilities <https://arxiv.org/html/2402.10835v1>`_
* `Time-LLM: Time Series Forecasting by Reprogramming Large Language Models <https://arxiv.org/abs/2310.01728>`_
* temps réel

Evaluation
==========

* https://defis.data.gouv.fr/
* le projet doit inclure au moins un graphe
  *Partial Dependence* ou *Permutation Importance* (voir liens ci-dessus)
* soutenance 11 avril 9h-13h
